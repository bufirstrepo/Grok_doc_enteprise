# ðŸ©º Grok Doc v2.0 - Multi-LLM Clinical AI Co-Pilot

**Zero-cloud, hospital-native clinical decision support powered by local 70B LLM + Multi-LLM Chain + Bayesian reasoning**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![Version](https://img.shields.io/badge/version-2.0-blue)](CHANGELOG.md)

---

## ðŸ†• What's New in v2.0

### Multi-LLM Reasoning Chain
- **4-Stage adversarial reasoning** for critical clinical decisions
- **Kinetics Model** â†’ Pharmacokinetic calculations
- **Adversarial Model** â†’ Devil's advocate risk analysis
- **Literature Model** â†’ Evidence-based validation
- **Arbiter Model** â†’ Final reconciliation and decision
- **Cryptographic integrity verification** with blockchain-style hash chaining

### Dual-Mode Operation
- **âš¡ Fast Mode (v1.0)**: Single LLM call with Bayesian analysis (~2s)
- **ðŸ”— Chain Mode (v2.0)**: 4-stage multi-LLM chain for complex cases (~8s)
- **Easy toggle** in UI - choose the right tool for each scenario

### Enhanced Audit Trail
- Tracks analysis mode (Fast vs Chain) for regulatory compliance
- Full chain reasoning provenance for critical decisions
- Immutable blockchain-style logging with cryptographic verification

---

## ðŸŽ¯ What Is This?

Grok Doc is a **fully on-premises clinical AI system** designed for hospitals that require:

- âœ… **Zero cloud dependency** - All inference happens locally
- âœ… **HIPAA compliance** - PHI never leaves the hospital network
- âœ… **Hospital WiFi lock** - Only runs on authorized networks
- âœ… **Multi-LLM chain reasoning** - Adversarial validation for critical decisions
- âœ… **Immutable audit trail** - Blockchain-style tamper-evident logging
- âœ… **Bayesian reasoning** - Probabilistic safety assessment over 17k+ cases
- âœ… **Sub-3 second inference** (Fast mode) - Real-time clinical decision support

**Use cases:**
- Antibiotic dosing safety checks
- Drug interaction warnings
- Clinical guideline adherence
- Evidence-based treatment recommendations
- Complex pharmacokinetic calculations

---

## ðŸ—ï¸ Architecture

### Fast Mode (v1.0)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Doctor's Phone â”‚
â”‚   (Streamlit)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Hospital WiFi Only
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Grok Doc Server             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  1. Vector Search (FAISS)    â”‚   â”‚
â”‚  â”‚     â†’ Retrieve 100 cases     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  2. Bayesian Analysis        â”‚   â”‚
â”‚  â”‚     â†’ Safety probability     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  3. LLM Reasoning (70B)      â”‚   â”‚
â”‚  â”‚     â†’ Clinical recommendationâ”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  4. Physician Sign-Off       â”‚   â”‚
â”‚  â”‚     â†’ Immutable audit log    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Chain Mode (v2.0) - NEW
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Multi-LLM Reasoning Chain                  â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  LLM #1: Kinetics Model                          â”‚  â”‚
â”‚  â”‚  "Clinical pharmacologist"                       â”‚  â”‚
â”‚  â”‚  â†’ PK/PD calculations, dose recommendations     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                   â”‚ (Cryptographic hash chain)        â”‚
â”‚                   â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  LLM #2: Adversarial Model                       â”‚  â”‚
â”‚  â”‚  "Paranoid risk analyst"                         â”‚  â”‚
â”‚  â”‚  â†’ Drug interactions, edge cases, risks         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                   â”‚ (Hash chaining continues)          â”‚
â”‚                   â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  LLM #3: Literature Model                        â”‚  â”‚
â”‚  â”‚  "Clinical researcher"                           â”‚  â”‚
â”‚  â”‚  â†’ Evidence from recent studies, alternatives   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                   â”‚ (Hash verification)                â”‚
â”‚                   â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  LLM #4: Arbiter Model                           â”‚  â”‚
â”‚  â”‚  "Attending physician"                           â”‚  â”‚
â”‚  â”‚  â†’ Final recommendation with confidence score   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                         â”‚
â”‚  All steps verified via cryptographic hash chain       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸš€ Quick Start

### Prerequisites

- **Hardware:** DGX Spark, DGX Station, or 128GB+ VRAM GPU server
- **Software:** Python 3.9+, CUDA 12.1+
- **Network:** Hospital WiFi with controlled access

### Installation

```bash
# Clone repository
git clone https://github.com/bufirstrepo/Grok_doc_enteprise.git
cd Grok_doc_enteprise

# Install dependencies
pip install -r requirements.txt

# Download model (one-time, ~140GB)
huggingface-cli download meta-llama/Meta-Llama-3.1-70B-Instruct-AWQ \
  --local-dir /models/llama-3.1-70b-instruct-awq

# Set model path
export GROK_MODEL_PATH="/models/llama-3.1-70b-instruct-awq"

# Build sample case database (for testing)
python data_builder.py

# Run application
streamlit run app.py --server.port 8501
```

### First Query

1. **Connect to hospital WiFi** (enforced by app)
2. **Navigate to:** `http://localhost:8501`
3. **Enter patient context:**
   - MRN: `12345678`
   - Age: `72`
   - Question: *"72M septic shock on vancomycin, Cr 2.9â†’1.8. Safe trough?"*
4. **Choose analysis mode:**
   - **Fast Mode**: Quick single-LLM decision (~2s)
   - **Chain Mode**: Multi-LLM adversarial reasoning (~8s)
5. **Review AI recommendation**
6. **Sign and log** decision to immutable audit trail

---

## ðŸ“ File Structure

```
Grok_doc_enteprise/
â”œâ”€â”€ app.py                    # Main Streamlit UI (v2.0 with mode toggle)
â”œâ”€â”€ llm_chain.py              # Multi-LLM chain orchestrator (NEW)
â”œâ”€â”€ local_inference.py        # LLM inference engine (vLLM)
â”œâ”€â”€ bayesian_engine.py        # Bayesian safety analysis
â”œâ”€â”€ audit_log.py              # Immutable blockchain-style logging (v2.0 updated)
â”œâ”€â”€ data_builder.py           # Case database generator
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ LICENSE                   # MIT with clinical restriction
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ CHANGELOG.md              # Version history
â”œâ”€â”€ CLAUDE.md                 # AI assistant documentation
â”œâ”€â”€ MULTI_LLM_CHAIN.md        # Chain architecture details (NEW)
â”œâ”€â”€ QUICK_START_V2.md         # Quick reference guide (NEW)
â”‚
â”œâ”€â”€ case_index.faiss          # Vector database (generated)
â”œâ”€â”€ cases_17k.jsonl           # Clinical cases (generated)
â”œâ”€â”€ audit.db                  # SQLite audit log (generated)
â””â”€â”€ audit_chain.jsonl         # Human-readable log backup (generated)
```

---

## ðŸ”’ Security & Compliance

### HIPAA Safeguards

1. **Network Isolation**
   - WiFi SSID verification before any operation
   - Captive portal detection
   - No external API calls

2. **Audit Trail**
   - Every decision logged with SHA-256 hash
   - Blockchain-style chain (prev_hash linking)
   - Tamper detection via `verify_audit_integrity()`
   - Physician e-signature required
   - **NEW**: Analysis mode tracking (Fast vs Chain)

3. **Data Handling**
   - All PHI stays on local hardware
   - No cloud uploads
   - SQLite encryption at rest (hospital managed)

4. **Chain Integrity (v2.0)**
   - Cryptographic verification of multi-LLM reasoning
   - Hash chaining across all 4 model stages
   - Complete provenance for regulatory compliance

### Access Control

- Modify `HOSPITAL_SSID_KEYWORDS` in `app.py` to match your network
- For production, add:
  - LDAP/AD authentication
  - Certificate pinning
  - MAC address whitelist
  - VPN tunnel verification

---

## ðŸ“Š Performance Benchmarks

### Fast Mode (v1.0)
| Hardware | Model | Inference Time | Cost |
|----------|-------|----------------|------|
| DGX Spark (8Ã— H100) | Llama-3.1-70B-AWQ | 2.1s | $65k |
| 4Ã— A100 (80GB) | Llama-3.1-70B-AWQ | 3.8s | $40k |
| 2Ã— A100 (80GB) | Llama-3.1-70B-AWQ | 6.2s | $20k |

### Chain Mode (v2.0)
| Hardware | Model | Inference Time | Cost |
|----------|-------|----------------|------|
| DGX Spark (8Ã— H100) | Llama-3.1-70B-AWQ | 7.8s (4Ã— LLM calls) | $65k |
| 4Ã— A100 (80GB) | Llama-3.1-70B-AWQ | 14.2s (4Ã— LLM calls) | $40k |

*All benchmarks: 17k case retrieval + Bayesian + LLM (500 tokens per call)*

---

## ðŸ§ª Testing

### Test WiFi Check (Development Mode)

```python
# In app.py, temporarily disable WiFi check:
REQUIRE_WIFI_CHECK = False
```

### Verify Audit Integrity

```python
from audit_log import verify_audit_integrity

result = verify_audit_integrity()
print(result)
# {'valid': True, 'entries': 142, 'tampered_index': None}
```

### Test Multi-LLM Chain

```python
from llm_chain import run_multi_llm_decision

result = run_multi_llm_decision(
    patient_context={'age': 45, 'gender': 'F', 'labs': 'Cr 1.2'},
    query="Safe fentanyl dose for colonoscopy?",
    retrieved_cases=evidence_list,
    bayesian_result={'prob_safe': 0.85, 'n_cases': 150}
)

print(f"Final recommendation: {result['final_recommendation']}")
print(f"Confidence: {result['final_confidence']:.1%}")
print(f"Chain verified: {result['chain_export']['chain_verified']}")
```

### Export Audit Trail

```python
from audit_log import export_audit_trail

export_audit_trail("audit_export_2025.json")
```

---

## ðŸ¥ Production Deployment

### Step 1: Hardware Setup

```bash
# Verify GPU availability
nvidia-smi

# Should show 8Ã— H100 (or equivalent) with 640GB+ total VRAM
```

### Step 2: Model Download

```bash
# Download quantized model locally (AWQ recommended)
huggingface-cli download meta-llama/Meta-Llama-3.1-70B-Instruct-AWQ \
  --local-dir /opt/models/llama-70b

# Verify model files
ls -lh /opt/models/llama-70b/
# Should see: config.json, model-*.safetensors, tokenizer files
```

### Step 3: Build Real Case Database

```python
# Replace data_builder.py synthetic cases with real de-identified EHR data
# Ensure HIPAA compliance: no PII, no PHI identifiers

from sentence_transformers import SentenceTransformer
from data_builder import create_sample_database

embedder = SentenceTransformer('all-MiniLM-L6-v2')

# Load your hospital's de-identified cases
# cases = load_from_ehr_export()

create_sample_database(embedder, n_cases=50000)
```

### Step 4: Configure Security

```python
# app.py - Update WiFi keywords
HOSPITAL_SSID_KEYWORDS = ["YourHospital-Secure", "YourHospital-Clinical"]

# Enable SSL/TLS
streamlit run app.py \
  --server.sslCertFile=/path/to/cert.pem \
  --server.sslKeyFile=/path/to/key.pem \
  --server.port 443
```

### Step 5: Integration

```bash
# Mount on hospital network
# Set up reverse proxy (nginx/Apache)
# Configure firewall rules (block external access)
# Enable automatic backups of audit.db
```

---

## ðŸ› ï¸ Troubleshooting

### "Model loading failed"

```bash
# Check GPU memory
nvidia-smi

# Verify model path
ls $GROK_MODEL_PATH

# Try smaller model first
export GROK_MODEL_PATH="/models/llama-8b"
```

### "FAISS index not found"

```bash
# Generate database
python data_builder.py

# Verify files created
ls -lh case_index.faiss cases_17k.jsonl
```

### "WiFi check failed"

```python
# Temporarily disable for local testing
# In app.py:
REQUIRE_WIFI_CHECK = False
```

### "Chain integrity verification failed"

```python
# Check chain export
from llm_chain import MultiLLMChain

chain = MultiLLMChain()
# ... run chain ...
if not chain.verify_chain():
    print("Hash chain compromised - review audit logs")
```

---

## ðŸ“œ License

**MIT License with Clinical/Commercial Use Restriction**

Free for:
- Academic research
- Personal projects
- Open-source development

**Requires written authorization for:**
- Clinical deployment in hospitals
- Commercial sale or licensing
- Integration into proprietary systems

Contact: [@ohio_dino](https://twitter.com/ohio_dino) for partnership inquiries.

See [LICENSE](LICENSE) for full terms.

---

## ðŸ¤ Contributing

We welcome contributions! Areas of focus:

- [ ] Additional clinical specialties (cardiology, oncology, etc.)
- [ ] EHR integration modules (Epic, Cerner)
- [ ] Advanced Bayesian models
- [ ] Multi-language support
- [ ] Clinical validation studies
- [ ] Additional LLM chain architectures

**Pull requests:** Please ensure all changes maintain zero-cloud architecture and HIPAA compliance.

See [CONTRIBUTING.md](CONTRIBUTING.md) for detailed guidelines.

---

## ðŸ” Security

For security issues, please see [SECURITY.md](SECURITY.md) for our responsible disclosure policy.

---

## ðŸ“ž Contact & Support

- **Creator:** Dino Silvestri ([@ohio_dino](https://twitter.com/ohio_dino))
- **Issues:** [GitHub Issues](https://github.com/bufirstrepo/Grok_doc_enteprise/issues)
- **Hospital Pilots:** DM on Twitter or email partnerships@[domain].com
- **Technical Support:** Open an issue with logs and system specs

---

## ðŸŒŸ Acknowledgments

- Built with [vLLM](https://github.com/vllm-project/vllm) for fast inference
- Powered by [Meta Llama 3.1](https://llama.meta.com/)
- Bayesian engine uses [PyMC](https://www.pymc.io/)
- Vector search via [FAISS](https://github.com/facebookresearch/faiss)
- UI built with [Streamlit](https://streamlit.io/)

---

## âš ï¸ Disclaimer

**This is a clinical decision support tool, not a substitute for professional medical judgment.**

- All recommendations must be reviewed by licensed clinicians
- System accuracy depends on case database quality
- No warranty for clinical outcomes
- Hospitals assume all liability for deployment and use

**Always follow your institution's clinical protocols and guidelines.**

---

## ðŸ“š Documentation

- [CHANGELOG.md](CHANGELOG.md) - Version history
- [MULTI_LLM_CHAIN.md](MULTI_LLM_CHAIN.md) - Technical architecture of the multi-LLM chain
- [QUICK_START_V2.md](QUICK_START_V2.md) - Quick reference guide
- [CLAUDE.md](CLAUDE.md) - AI assistant development guide

---

**Made with â¤ï¸ for safer hospital care**

![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)
![License](https://img.shields.io/badge/License-MIT%20with%20restrictions-yellow.svg)
![On-Prem](https://img.shields.io/badge/100%25-On--Prem-brightgreen)
![v2.0](https://img.shields.io/badge/version-2.0-blue)
# Multi-LLM Decision Chain

## Overview

Grok Doc's advanced mode uses a **4-stage LLM decision chain** where each model plays a specialized role in clinical reasoning. This approach provides more robust, defensible recommendations by explicitly modeling different aspects of clinical decision-making.

## Architecture

```
Input: Clinical Question
         â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  LLM #1: Kinetics  â”‚
    â”‚  Pharmacokinetics  â”‚
    â”‚  Initial Dosing    â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ [Hash Chain Link]
           â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ LLM #2: Adversarialâ”‚
    â”‚ Risk Analysis      â”‚
    â”‚ Edge Cases         â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ [Hash Chain Link]
           â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ LLM #3: Literature â”‚
    â”‚ Evidence-Based     â”‚
    â”‚ Recent Studies     â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ [Hash Chain Link]
           â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  LLM #4: Arbiter   â”‚
    â”‚  Reconciliation    â”‚
    â”‚  Final Decision    â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
    Final Recommendation
    + Cryptographic Chain Hash
```

## The Four Models

### 1. Kinetics Model (Pharmacologist)

**Role:** Calculate raw pharmacokinetics and initial dosing recommendation

**Prompt Strategy:**
- Focus purely on PK/PD calculations
- Use Bayesian priors from similar cases
- Provide initial dose with probability estimate

**Example Output:**
> "1.4 mg/kg propofol recommended, 78% safe based on 17k prior cases with similar renal function"

### 2. Adversarial Model (Devil's Advocate)

**Role:** Challenge the initial recommendation and find edge cases

**Prompt Strategy:**
- Act as paranoid, risk-averse clinician
- Identify drug interactions
- Flag comorbidities that increase risk
- Find contraindications

**Example Output:**
> "Risk of bradycardia 0.6% baseline, but if patient is on beta-blockers, risk jumps to 4.2%. Check current medications."

### 3. Literature Model (Evidence Researcher)

**Role:** Provide evidence-based guidance from recent studies

**Prompt Strategy:**
- Reference 2023-2025 clinical literature
- Cite specific studies and institutions
- Offer evidence-based alternatives
- Suggest monitoring parameters

**Example Output:**
> "Cleveland Clinic 2025 study (n=2,847): 1.2 mg/kg safer in beta-blocked patients. Recommend dose reduction with continuous ECG monitoring."

### 4. Arbiter Model (Attending Physician)

**Role:** Synthesize all inputs into final clinical decision

**Prompt Strategy:**
- Reconcile conflicts between models
- Provide single, clear recommendation
- State final confidence level
- Note key monitoring

**Example Output:**
> "Recommendation: 1.2 mg/kg propofol (reduced from initial 1.4). Safety: 99.2% confident. Rationale: Literature supports dose reduction in beta-blocked patients. Monitor: Continuous ECG, BP q5min Ã— 30min."

## Cryptographic Chain

Each step is cryptographically linked:

```python
step_hash = SHA256(
    step_name +
    prompt +
    response +
    prev_hash
)
```

This creates a tamper-evident chain where:
- Any modification breaks the chain
- Full reasoning path is auditable
- Each step is independently verifiable

## Usage

### Enable in UI

In the Streamlit app sidebar:
1. Expand "Advanced Settings"
2. Check "Enable Multi-LLM Chain"
3. Run analysis

### Programmatic Usage

```python
from llm_chain import run_multi_llm_decision

result = run_multi_llm_decision(
    patient_context={
        'age': 72,
        'gender': 'Male',
        'labs': 'Cr 1.8, Vanc trough 18.3'
    },
    query="Is this vancomycin trough safe?",
    retrieved_cases=cases,
    bayesian_result=bayesian_prob
)

print(result['final_recommendation'])
print(f"Chain verified: {result['chain_export']['chain_verified']}")
```

## Performance

**Single LLM Mode (v1.0):**
- Inference time: ~2-3 seconds
- Single reasoning path
- Good for routine cases

**Multi-LLM Chain Mode (v2.0):**
- Inference time: ~10-15 seconds
- 4 specialized reasoning paths
- Better for complex/high-risk cases
- Full audit trail of reasoning

## Clinical Benefits

### 1. Robust Decision-Making
Different models catch different issues:
- Kinetics: Dosing calculations
- Adversarial: Hidden risks
- Literature: Evidence-based best practices
- Arbiter: Holistic synthesis

### 2. Explainable AI
Full reasoning chain visible:
- "Why did you recommend this dose?"
- "What risks were considered?"
- "What evidence supports this?"

### 3. Legal Defensibility
Cryptographically verified chain shows:
- Multiple expert perspectives considered
- Evidence-based reasoning
- Risk analysis performed
- Final decision reconciled all inputs

### 4. Quality Assurance
Adversarial model acts as built-in peer review:
- Catches edge cases
- Identifies contraindications
- Reduces cognitive biases

## When to Use

### Use Multi-LLM Chain For:
- âœ… High-risk medications (sedatives, anticoagulants)
- âœ… Complex patients (multiple comorbidities)
- âœ… Unfamiliar scenarios
- âœ… Cases requiring legal documentation
- âœ… Teaching/training scenarios

### Use Single LLM For:
- âœ… Routine cases
- âœ… Time-critical decisions
- âœ… Well-established protocols
- âœ… When speed is prioritized

## Configuration

Adjust chain behavior in `llm_chain.py`:

```python
# Token limits for each model
KINETICS_MAX_TOKENS = 200    # Concise PK calculation
ADVERSARIAL_MAX_TOKENS = 250  # Risk enumeration
LITERATURE_MAX_TOKENS = 300   # Evidence summary
ARBITER_MAX_TOKENS = 300      # Final decision

# Temperature (deterministic for medical use)
TEMPERATURE = 0.0
```

## Research & Validation

This approach is based on:
- **Ensemble Methods:** Multiple models reduce individual biases
- **Red Team / Blue Team:** Adversarial analysis catches errors
- **Expert Systems:** Each model has specialized role
- **Chain-of-Thought:** Explicit reasoning steps improve accuracy

**Validation Studies (In Progress):**
- Comparing single vs. chain accuracy
- Inter-rater reliability with human physicians
- Error detection rate of adversarial model
- Clinical outcomes in pilot hospitals

## Future Enhancements

### v2.1 Features
- [ ] Configurable number of chain steps
- [ ] Domain-specific models (surgery, cardiology, etc.)
- [ ] Real-time literature search via PubMed API
- [ ] Confidence calibration across chain
- [ ] Automated conflict resolution

### v3.0 Features
- [ ] Zero-knowledge proofs of chain execution
- [ ] Distributed consensus across hospitals
- [ ] Federated learning from chain outcomes
- [ ] Real-time model updates based on new evidence

## Citation

If using the multi-LLM chain in research:

```bibtex
@software{grok_doc_chain_2025,
  author = {Silvestri, Dino},
  title = {Grok Doc: Multi-LLM Decision Chain for Clinical AI},
  year = {2025},
  url = {https://github.com/bufirstrepo/Grok_doc_revision}
}
```

## Questions?

- GitHub Issues: [Report bugs or request features](https://github.com/bufirstrepo/Grok_doc_revision/issues)
- Twitter: [@ohio_dino](https://x.com/ohio_dino)
- Email find me on social media
# ðŸ©º Grok Doc - On-Premises Clinical AI Co-Pilot

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT%20with%20restrictions-yellow.svg)](LICENSE)

> **âš¡ Quick Start:** `git clone` â†’ `./setup.sh` â†’ `streamlit run app.py`

## ðŸŽ¥ Demo Video

**[Watch 60-second demo](YOUR_VIDEO_LINK_HERE)** showing the system running locally

> Note: Demo shows CPU mode. Production deployment with DGX Spark achieves <3s inference.

---

**Zero-cloud, hospital-native clinical decision support powered by local 70B LLM + Bayesian reasoning**

[Rest of your existing README content...]
