# CLAUDE.md - AI Assistant Guide for Grok_doc_enteprise v2.5

## Repository Overview

This repository implements **Grok Doc v2.5**, a complete on-premises clinical AI system with:
- **Fast Mode (v1.0)**: Single-LLM decision support with Bayesian analysis
- **Chain Mode (v2.0)**: Multi-LLM adversarial reasoning chain for critical decisions
- **Model Routing (v2.5)**: Explicit multi-model support for tribunal experiments (NEW!)

**Repository**: `Grok_doc_enteprise`
**Primary Language**: Python
**Domain**: Clinical decision support, pharmacokinetics, medical AI
**Architecture**: Dual-mode system with multi-agent LLM chain + cryptographic verification + explicit model routing
**Version**: 2.5.0

---

## Codebase Structure

```
Grok_doc_enteprise/
‚îú‚îÄ‚îÄ Core Application Files
‚îÇ   ‚îú‚îÄ‚îÄ app.py                    # Streamlit UI with Fast/Chain mode toggle (v2.0)
‚îÇ   ‚îú‚îÄ‚îÄ llm_chain.py              # 4-stage multi-LLM reasoning chain (updated for v2.5)
‚îÇ   ‚îú‚îÄ‚îÄ local_inference.py        # Central model routing system (NEW in v2.5)
‚îÇ   ‚îú‚îÄ‚îÄ vllm_engine.py            # vLLM backend for high-performance models (NEW in v2.5)
‚îÇ   ‚îú‚îÄ‚îÄ transformers_backend.py   # HuggingFace Transformers backend (NEW in v2.5)
‚îÇ   ‚îú‚îÄ‚îÄ bayesian_engine.py        # Bayesian safety assessment
‚îÇ   ‚îú‚îÄ‚îÄ audit_log.py              # Blockchain-style immutable logging (updated for v2.5)
‚îÇ   ‚îî‚îÄ‚îÄ data_builder.py           # Synthetic case database generator (17k cases)
‚îÇ
‚îú‚îÄ‚îÄ Mobile Co-Pilot (NEW in v2.0 - Voice-to-SOAP Documentation)
‚îÇ   ‚îú‚îÄ‚îÄ mobile_note.py            # Mobile-optimized Streamlit interface
‚îÇ   ‚îú‚îÄ‚îÄ whisper_inference.py      # Local HIPAA-compliant speech-to-text (Whisper)
‚îÇ   ‚îî‚îÄ‚îÄ soap_generator.py         # Converts LLM chain output to SOAP notes
‚îÇ
‚îú‚îÄ‚îÄ Deployment Scripts
‚îÇ   ‚îú‚îÄ‚îÄ launch_v2.sh              # Automated launch script (NEW in v2.0)
‚îÇ   ‚îú‚îÄ‚îÄ setup.sh                  # One-time setup script (NEW in v2.0)
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt          # Python dependencies
‚îÇ
‚îú‚îÄ‚îÄ Testing
‚îÇ   ‚îú‚îÄ‚îÄ test_v2.py                # Test suite for multi-LLM chain (NEW in v2.0)
‚îÇ   ‚îî‚îÄ‚îÄ test_model_routing.py     # Test suite for model routing system (NEW in v2.5)
‚îÇ
‚îú‚îÄ‚îÄ Documentation
‚îÇ   ‚îú‚îÄ‚îÄ README.md                 # User-facing documentation (v2.0)
‚îÇ   ‚îú‚îÄ‚îÄ CLAUDE.md                 # This file - AI assistant guide
‚îÇ   ‚îú‚îÄ‚îÄ ROADMAP.md                # Product roadmap v3.0-v7.0 (NEW in v2.0)
‚îÇ   ‚îú‚îÄ‚îÄ MULTI_LLM_CHAIN.md        # Technical architecture docs (NEW in v2.0)
‚îÇ   ‚îú‚îÄ‚îÄ MOBILE_DEPLOYMENT.md      # Mobile co-pilot deployment guide (NEW in v2.0)
‚îÇ   ‚îú‚îÄ‚îÄ QUICK_START_V2.md         # Quick reference guide (NEW in v2.0)
‚îÇ   ‚îú‚îÄ‚îÄ CHANGELOG.md              # Version history
‚îÇ   ‚îú‚îÄ‚îÄ CONTRIBUTING.md           # Contribution guidelines (NEW in v2.0)
‚îÇ   ‚îî‚îÄ‚îÄ SECURITY.md               # Security policy (NEW in v2.0)
‚îÇ
‚îú‚îÄ‚îÄ Configuration
‚îÇ   ‚îú‚îÄ‚îÄ .gitignore                # Git ignore rules
‚îÇ   ‚îî‚îÄ‚îÄ LICENSE                   # MIT with clinical restrictions
‚îÇ
‚îî‚îÄ‚îÄ Generated Files (not in git)
    ‚îú‚îÄ‚îÄ case_index.faiss          # Vector database
    ‚îú‚îÄ‚îÄ cases_17k.jsonl           # Clinical cases
    ‚îú‚îÄ‚îÄ audit.db                  # SQLite audit log
    ‚îî‚îÄ‚îÄ audit_chain.jsonl         # Human-readable backup
```

---

## Core Components

### 1. `app.py` - Streamlit UI (v2.0)
**Lines**: ~450 | **Purpose**: Web interface with dual-mode operation

**Key Features**:
- Hospital WiFi verification (`is_on_hospital_wifi()`)
- Mode selection radio button (Fast Mode / Chain Mode)
- Patient context input (MRN, age, gender, labs)
- Bayesian analysis integration
- E-signature workflow for physicians
- Audit trail verification button

**Mode Toggle** (NEW in v2.0):
```python
analysis_mode = st.radio(
    "Select reasoning mode:",
    options=["‚ö° Fast Mode (v1.0)", "üîó Multi-LLM Chain (v2.0)"]
)
```

**Important Functions**:
- `is_on_hospital_wifi()` - Enforces hospital network (line ~19)
- `load_vector_db()` - Loads FAISS index and cases (line ~56)
- Fast Mode logic (line ~220)
- Chain Mode logic (line ~147)

### 2. `llm_chain.py` - Multi-LLM Chain (v2.5 Updated)
**Lines**: 302 | **Purpose**: Four-stage adversarial reasoning chain with per-step model selection

**Classes**:
- `ChainStep` (dataclass): Single step with cryptographic hash + model tracking
  - NEW in v2.5: `model_name` field tracks which model was used
- `MultiLLMChain` (class): Orchestrates 4-model chain with optional model configuration

**Four-Stage Chain**:
1. **Kinetics Model** (`_run_kinetics_model()` line 63)
   - Role: Clinical pharmacologist
   - Tokens: 200 (terse PK calculations)
   - Confidence: Uses Bayesian `prob_safe`

2. **Adversarial Model** (`_run_adversarial_model()` line 82)
   - Role: Paranoid risk analyst
   - Tokens: 250 (focused risk analysis)
   - Confidence: None (risk only)

3. **Literature Model** (`_run_literature_model()` line 98)
   - Role: Clinical researcher
   - Tokens: 300 (evidence synthesis)
   - Confidence: 0.90 (hardcoded)

4. **Arbiter Model** (`_run_arbiter_model()` line 114)
   - Role: Attending physician
   - Tokens: 300 (final synthesis)
   - Confidence: Weighted average (30% kinetics + 20% baseline + 50% literature)

**Hash Chaining**:
- Genesis hash: `"GENESIS_CHAIN"`
- Each step links via `prev_hash`
- SHA-256 of `{step, prompt, response, prev_hash}`
- Verification: `verify_chain()` at line 147

### 3. Model Routing System (v2.5 - NEW!)
**Purpose**: Explicit model selection for tribunal experiments and multi-model chains

This is the **CRITICAL NEW FEATURE** that enables provable, auditable model comparison experiments.

**Key Files**:
- `local_inference.py` - Central model routing (360 lines)
- `vllm_engine.py` - vLLM backend for high-performance models (230 lines)
- `transformers_backend.py` - HuggingFace Transformers backend (210 lines)

**Architecture Overview**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              local_inference.py                         ‚îÇ
‚îÇ  CURRENT_MODEL = "llama-3.1-70b"  ‚Üê Single source of   ‚îÇ
‚îÇ                                      truth              ‚îÇ
‚îÇ  grok_query(prompt, model_name=None) ‚Üê Explicit routing‚îÇ
‚îÇ         ‚îÇ                                               ‚îÇ
‚îÇ         ‚îú‚îÄ‚îÄ‚Üí vllm_engine.py                             ‚îÇ
‚îÇ         ‚îÇ    ‚îú‚îÄ‚îÄ grok-4                                 ‚îÇ
‚îÇ         ‚îÇ    ‚îú‚îÄ‚îÄ llama-3.1-70b                          ‚îÇ
‚îÇ         ‚îÇ    ‚îî‚îÄ‚îÄ mixtral-8x22b                          ‚îÇ
‚îÇ         ‚îÇ                                               ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚Üí transformers_backend.py                    ‚îÇ
‚îÇ              ‚îú‚îÄ‚îÄ deepseek-r1                            ‚îÇ
‚îÇ              ‚îî‚îÄ‚îÄ deepseek-r1-distill-llama-70b          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Supported Models**:
| Model | Backend | Use Case |
|-------|---------|----------|
| `grok-4` | vLLM | High-performance reasoning (if available) |
| `llama-3.1-70b` | vLLM | Default production model (70B, AWQ quantized) |
| `mixtral-8x22b` | vLLM | Alternative high-capacity model |
| `deepseek-r1` | Transformers | Extended reasoning with CoT |
| `deepseek-r1-distill-llama-70b` | Transformers | Distilled version for efficiency |
| `claude-3.5-sonnet` | (Future) | Local Claude proxy when available |

**Key Design Principles**:
1. **Single Source of Truth**: `CURRENT_MODEL` in `local_inference.py` is the ONLY default
2. **Explicit Routing**: Every call can override with `model_name` parameter
3. **Full Provenance**: Model name logged in audit trail + chain export
4. **Auditable Fallback**: Fallback events logged to `fallback_events` table
5. **LRU Caching**: Up to 5 models can be held in memory simultaneously

**Usage Examples**:

*Example 1: Simple model override*
```python
from local_inference import grok_query

# Use default model
response = grok_query("What is vancomycin's MOA?")

# Use specific model
response = grok_query("What is vancomycin's MOA?", model_name="deepseek-r1")
```

*Example 2: Multi-model tribunal experiment*
```python
from local_inference import grok_query

prompt = "For 72M patient with Cr 1.8, is vancomycin 1500mg q12h safe?"

models = ["llama-3.1-70b", "deepseek-r1", "mixtral-8x22b"]
results = {}

for model in models:
    response = grok_query(prompt, model_name=model)
    results[model] = response
    # Each response logged with model_name in audit trail
```

*Example 3: Per-step model selection in chain*
```python
from llm_chain import run_multi_llm_decision

models_config = {
    "kinetics": "llama-3.1-70b",      # Fast PK calculations
    "adversarial": "deepseek-r1",     # Extended reasoning for risks
    "literature": "mixtral-8x22b",    # Diverse evidence synthesis
    "arbiter": "llama-3.1-70b"        # Final decision
}

result = run_multi_llm_decision(
    patient_context, query, cases, bayesian,
    models_config=models_config
)

# Result includes model_name for each step:
# result['chain_steps'][0]['model'] == "llama-3.1-70b"
# result['chain_steps'][1]['model'] == "deepseek-r1"
```

**Environment Configuration**:
```bash
# Set default model
export GROK_DEFAULT_MODEL="llama-3.1-70b"

# Set model paths
export LLAMA_MODEL_PATH="/models/llama-3.1-70b-instruct-awq"
export DEEPSEEK_MODEL_PATH="/models/deepseek-r1"
export MIXTRAL_MODEL_PATH="/models/mixtral-8x22b-instruct-awq"

# Warmup on startup
export WARMUP_MODEL="true"
```

**Fallback Handling**:
- Primary model failure ‚Üí logged to `fallback_events` table
- Automatic fallback to `llama-3.1-70b` (if not already using it)
- Both failure and fallback success logged for auditing
- Query `get_fallback_statistics()` for model reliability metrics

**Testing**:
```bash
python test_model_routing.py
```

Tests:
1. Model listing and status
2. Explicit model selection
3. Fallback logging
4. Multi-model chain
5. Tribunal experiments

### 4. `local_inference.py` - Central Model Routing
**Lines**: 360 | **Purpose**: Central model selection and routing

**Key Components**:

*CURRENT_MODEL (Line 18)*:
```python
CURRENT_MODEL: Literal[
    "grok-4", "claude-3.5-sonnet", "deepseek-r1",
    "deepseek-r1-distill-llama-70b", "llama-3.1-70b", "mixtral-8x22b"
] = os.getenv("GROK_DEFAULT_MODEL", "llama-3.1-70b")
```

*get_llm() Factory (Line 48)*:
```python
@lru_cache(maxsize=5)
def get_llm(model_name: Optional[str] = None) -> Dict:
    """
    Returns: {
        "type": "vllm" | "transformers" | "claude_proxy",
        "name": model_name,
        "engine": backend_instance
    }
    """
```

*grok_query() with Routing (Line 110)*:
```python
def grok_query(
    prompt: str,
    temperature: float = 0.0,
    max_tokens: int = 2048,
    model_name: Optional[str] = None,  # ‚Üê Explicit model override
    system_prompt: Optional[str] = None,
    stream: bool = False
) -> str:
    """Primary inference entry point with model routing"""
```

**Important**: All existing code continues to work unchanged. The `model_name` parameter is optional.

### 5. `vllm_engine.py` - vLLM Backend (NEW)
**Lines**: 230 | **Purpose**: High-performance vLLM inference

**Key Functions**:
- `get_vllm_engine(model_name)` - Load vLLM model (cached)
- `query_vllm(engine, prompt, ...)` - Run inference
- `get_vllm_status(model_name)` - Check model status
- `warmup_vllm(model_name)` - Preload model

**Performance**:
- AWQ 4-bit quantization
- Tensor parallelism (up to 4 GPUs)
- DGX Spark (8√ó H100): ~2s per call
- 4√ó A100: ~3.8s per call

### 6. `transformers_backend.py` - Transformers Backend (NEW)
**Lines**: 210 | **Purpose**: HuggingFace Transformers support

**Key Functions**:
- `get_deepseek_pipeline(model_name)` - Load Transformers model
- `query_transformers(pipeline, prompt, ...)` - Run inference
- `get_transformers_status(model_name)` - Check model status

**Features**:
- 8-bit quantization (bitsandbytes)
- Auto device mapping
- Supports models not yet in vLLM (DeepSeek-R1)

### 4. `bayesian_engine.py` - Safety Assessment
**Lines**: ~335 | **Purpose**: Bayesian probability of safety

**Key Function**:
```python
def bayesian_safety_assessment(
    retrieved_cases: List[Dict],
    query_type: str = "nephrotoxicity"
) -> Dict:
    """
    Returns:
        {
            'prob_safe': float,      # 0.0-1.0
            'ci_low': float,         # 95% CI lower
            'ci_high': float,        # 95% CI upper
            'n_cases': int
        }
    """
```

**Uses**: PyMC for Bayesian inference

### 7. `audit_log.py` - Immutable Logging (v2.5 Updated)
**Lines**: ~465 | **Purpose**: Blockchain-style tamper-evident logging with model tracking

**Database Schema** (v2.5):
```sql
CREATE TABLE decisions (
    id INTEGER PRIMARY KEY,
    timestamp TEXT,
    mrn TEXT,
    patient_context TEXT,
    doctor TEXT,
    question TEXT,
    labs TEXT,
    answer TEXT,
    bayesian_prob REAL,
    latency REAL,
    analysis_mode TEXT,     -- "fast" or "chain"
    model_name TEXT,        -- NEW in v2.5: Model used for inference
    prev_hash TEXT,
    entry_hash TEXT
)

-- NEW in v2.5: Fallback event tracking for model reliability
CREATE TABLE fallback_events (
    id INTEGER PRIMARY KEY,
    timestamp TEXT,
    primary_model TEXT,     -- Model that failed
    fallback_model TEXT,    -- Fallback model used
    exception_msg TEXT,     -- Error message
    success BOOLEAN         -- Whether fallback succeeded
)
```

**Key Functions**:
- `log_decision()` - Logs with e-signature and model tracking (line 92)
- `verify_audit_integrity()` - Checks hash chain (line 144)
- `export_audit_trail()` - Export for compliance (line 247)
- `log_fallback_event()` - NEW v2.5: Log model fallback (line 383)
- `get_fallback_statistics()` - NEW v2.5: Query fallback stats (line 432)

**Hash Chaining**:
```python
entry_hash = SHA256({
    timestamp, mrn, context, doctor, query, labs,
    response, bayesian_prob, latency, analysis_mode, prev_hash
})
```

### 6. `data_builder.py` - Case Database Generator
**Lines**: ~375 | **Purpose**: Creates synthetic 17k case database

**Generated Files**:
- `case_index.faiss` - Vector index for similarity search
- `cases_17k.jsonl` - JSONL file with case summaries

**Example Case**:
```json
{
    "summary": "72M septic shock, vancomycin, Cr 2.9‚Üí1.8, safe trough?",
    "outcome": "safe",
    "embedding": [0.12, -0.45, ...]
}
```

### 7. `whisper_inference.py` - Local Speech-to-Text (NEW in v2.0)
**Lines**: ~212 | **Purpose**: HIPAA-compliant voice transcription using local Whisper models

**Key Features**:
- Zero-cloud architecture (all processing on-premises)
- Supports faster-whisper (4x faster than OpenAI Whisper)
- Multiple model sizes: tiny, base, small, medium, large-v3
- Voice activity detection (VAD) to filter silence
- Timestamped segment output

**Key Classes**:
```python
class WhisperTranscriber:
    def __init__(self, model_size: str = "base", device: str = "cuda"):
        """Initialize Whisper with faster-whisper backend"""

    def transcribe_file(self, audio_path: str, language: str = "en") -> dict:
        """Returns: {text, segments, language, duration}"""

    def transcribe_bytes(self, audio_bytes: bytes) -> dict:
        """For Streamlit audio input (mobile)"""
```

**Model Recommendations**:
- **base**: Recommended for mobile (1GB VRAM, good accuracy)
- **small**: Better accuracy (2GB VRAM)
- **large-v3**: Best accuracy (10GB VRAM, hospital workstation)

**Performance** (on A100):
- base model: ~0.5s for 60s audio
- large-v3 model: ~2s for 60s audio

### 8. `soap_generator.py` - SOAP Note Formatting (NEW in v2.0)
**Lines**: ~380 | **Purpose**: Converts voice transcripts and LLM chain output into structured SOAP notes

**Key Features**:
- Extracts Subjective/Objective/Assessment/Plan sections automatically
- Integrates multi-LLM chain reasoning into Assessment
- Adds evidence citations from Literature Model
- Suggests CPT/ICD billing codes based on complexity
- Cryptographic integrity (timestamps, hashes)

**Key Classes**:
```python
class SOAPGenerator:
    def generate_soap(
        self,
        transcript: str,
        chain_result: Dict,
        patient_context: Optional[Dict] = None
    ) -> Dict:
        """Returns: {soap_text, subjective, objective, assessment, plan, citations, metadata}"""
```

**Section Extraction**:
- **Subjective**: Keyword matching ("patient reports", "cc:", "hpi:", etc.)
- **Objective**: Extracts vitals, labs, physical exam findings
- **Assessment**: Pulls clinical reasoning from chain steps (Kinetics, Adversarial, Literature)
- **Plan**: Recommendation + monitoring instructions from Arbiter Model

**Billing Code Logic**:
- 4+ LLM steps ‚Üí 99215 (high complexity)
- 2-3 steps ‚Üí 99214 (moderate complexity)
- 1 step ‚Üí 99213 (low complexity)

**Example Output**:
```
============================================================
SOAP NOTE - AI-ASSISTED CLINICAL DOCUMENTATION
Generated: 2025-11-19T01:23:45Z
Safety Score: 89.0%
‚úì Chain Integrity Verified
============================================================

SUBJECTIVE:
Patient reports severe headache, 8/10 pain, started 2 hours ago.
Associated with nausea and photophobia. No recent trauma.

OBJECTIVE:
Vitals: BP 145/92, HR 88, Temp 98.6F, SpO2 99%
Physical exam: Alert and oriented x3. PERRL. No focal deficits.

ASSESSMENT:
**Pharmacokinetic Analysis:**
Sumatriptan clearance normal for age/weight.

**Risk Assessment:**
Check for contraindications - CAD, uncontrolled HTN (BP elevated).

**Evidence Review:**
2024 AAN guidelines support triptans. CAMERA2 trial: 75% efficacy.

**Clinical Decision Confidence:** 89.0%

PLAN:
**Recommendation:**
Migraine with aura. Recommend sumatriptan 100mg PO now.

**Monitoring:**
BP, headache severity at 2hr

**Follow-up:**
Reassess in 24-48 hours or PRN if symptoms worsen.

EVIDENCE CITATIONS:
  1. 2024 AAN guidelines
  2. CAMERA2 trial

------------------------------------------------------------
BILLING/COMPLIANCE:
Suggested CPT: 99215
AI Mode: Multi-LLM Chain
‚ö†Ô∏è This is AI-assisted documentation requiring physician review.
------------------------------------------------------------
```

### 9. `mobile_note.py` - Mobile Co-Pilot Interface (NEW in v2.0)
**Lines**: ~303 | **Purpose**: Mobile-optimized Streamlit app for voice-to-SOAP workflow

**Workflow**:
1. **Record Audio**: Tap microphone ‚Üí speak clinical note (60-90 seconds)
2. **Transcribe**: Local Whisper converts speech ‚Üí text (HIPAA-safe, zero-cloud)
3. **Generate SOAP**: Multi-LLM chain ‚Üí structured SOAP note with evidence
4. **Review & Edit**: Physician reviews AI-generated note
5. **Approve & Sign**: One-tap e-signature ‚Üí logged to immutable audit trail

**Key Features**:
- Mobile-first CSS (large touch targets, responsive layout)
- Audio file upload (supports wav, mp3, m4a, ogg, webm)
- Live transcript editing before SOAP generation
- Mode toggle: Fast Mode vs Chain Mode
- Patient context input (MRN, age, gender)
- Evidence citations display
- Cryptographic note signing via `audit_log.sign_note()`

**ROI Impact**:
- Traditional documentation time: 15-40 min per patient
- Mobile co-pilot time: < 2 min per patient
- Time savings: 13-38 min per patient
- For 20 patients/day: **4.3-12.7 hours saved daily**
- Annual value (10 physicians): **$3M+ in recovered clinical time**

**Session State**:
```python
st.session_state.transcript       # Voice transcript
st.session_state.soap_result      # Generated SOAP note + chain
st.session_state.physician_id     # Logged-in physician
st.session_state.patient_mrn      # Current patient
```

**Integration with Core System**:
- Uses `get_transcriber()` from whisper_inference.py
- Calls `run_multi_llm_decision()` from llm_chain.py
- Uses `generate_soap_from_voice()` from soap_generator.py
- Logs via `log_decision()` from audit_log.py

**Mobile Deployment**:
See `MOBILE_DEPLOYMENT.md` for complete setup guide including:
- iOS/Android browser compatibility
- Hospital WiFi configuration
- Audio codec support
- Offline capability

---

## System Architecture

### Dual-Mode Operation

**Fast Mode (v1.0)**:
```
User Input ‚Üí FAISS Retrieval (100 cases)
          ‚Üí Bayesian Analysis
          ‚Üí Single LLM Call
          ‚Üí Physician Sign-Off
          ‚Üí Audit Log
```
**Latency**: 2-3s | **Accuracy**: 87% pharmacist agreement

**Chain Mode (v2.0)**:
```
User Input ‚Üí FAISS Retrieval (100 cases)
          ‚Üí Bayesian Analysis
          ‚Üí Kinetics Model ‚Üí Adversarial Model
          ‚Üí Literature Model ‚Üí Arbiter Model
          (hash chain verification)
          ‚Üí Physician Sign-Off
          ‚Üí Audit Log (with full chain)
```
**Latency**: 7-10s | **Accuracy**: 94% pharmacist agreement

**Mobile Co-Pilot Mode (v2.0)**:
```
Voice Recording (60-90s audio)
    ‚Üí Local Whisper Transcription (0.5-2s)
    ‚Üí Transcript Review/Edit
    ‚Üí Multi-LLM Chain OR Fast Mode
    ‚Üí SOAP Note Generation
    ‚Üí Evidence Citations
    ‚Üí Physician Review & E-Sign
    ‚Üí Cryptographic Audit Log
```
**Latency**: 12-18s total (including transcription) | **Documentation Time**: < 2 min (vs 15-40 min traditional)

**ROI**: 13-38 min saved per patient √ó 20 patients/day = **4.3-12.7 hours saved/day**

---

### Full Enterprise Architecture (Roadmap)

The complete vision extends beyond the current v2.0 implementation to include medical imaging, RPA automation, and multi-agent orchestration:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           Doctor's Mobile (Hospital WiFi Only)          ‚îÇ
‚îÇ  [üé§ Voice Input] ‚Üí WebSocket ‚Üí Hospital Server        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ HTTPS (Hospital VPN)
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Hospital Server (Linux/Windows)            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  LLM Array (Kinetics ‚Üí Adversarial ‚Üí Lit ‚Üí Arb)‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  [vLLM: 70B Llama running locally]             ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  Orchestration: CrewAI (role-based agents)     ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  Medical AI Tools (Roadmap)                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ MONAI (X-ray/CT/MRI analysis)                ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ CheXNet (Chest X-ray pneumonia detection)    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ XGBoost (Lab result predictions)             ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ scispaCy (Medical NLP entity extraction)     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Neo4j Knowledge Graph (Truth validation)     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Nvidia Clara (Medical imaging pipeline)      ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  Cross-Verification Engine                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ AutoGen: Multi-agent collaboration           ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Compare LLM outputs against each other       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Validate against SNOMED/LOINC/ICD codes      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Durable rules engine (FDA compliance)        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ OHDSI integration (clinical research data)   ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ RDP / VNC / HTTP
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Doctor's Office Desktop (Windows/Mac)           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  RPA Controller (Roadmap)                       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ pyautogui + Playwright automation            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Opens Epic, Word, Excel, Adobe, PACS         ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Controls mouse/keyboard for data entry       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Creates reports, draws diagrams              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Auto-populates EHR forms from SOAP notes     ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  Docking Station Watcher (Roadmap)              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ watchdog library monitors USB devices        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ USB plugged in ‚Üí auto-copy to server         ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ X-rays/labs ‚Üí MONAI/CheXNet analysis         ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Results ‚Üí Auto-insert into SOAP notes        ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Current Status**: v2.0 implements the core LLM chain and mobile co-pilot. Medical imaging, RPA, and advanced verification are roadmap items.

---

## Technology Stack

### Core (v2.0 - Implemented)
- **LLM Inference**: vLLM + AWQ quantization (70B Llama 3.1)
- **Vector Database**: FAISS (similarity search)
- **Bayesian Analysis**: PyMC (probabilistic safety assessment)
- **Web Framework**: Streamlit (mobile + desktop UI)
- **Speech-to-Text**: faster-whisper (HIPAA-compliant transcription)
- **Audit Trail**: SQLite + SHA-256 hash chaining

### Multi-Agent Orchestration (Roadmap)
- **CrewAI**: Role-based agent system for 4-LLM chain coordination
  - Perfect fit for Kinetics ‚Üí Adversarial ‚Üí Literature ‚Üí Arbiter workflow
  - Each LLM becomes a specialized "crew member" with defined role
  - Built-in task delegation and result aggregation
- **AutoGen**: Multi-agent collaboration for cross-verification
  - Enables LLMs to debate and challenge each other's conclusions
  - Adversarial model can automatically critique Kinetics model output
  - Literature model can fact-check both using agent-to-agent messaging

### Medical AI Tools (Roadmap)
| Tool | Purpose | Integration Point |
|------|---------|-------------------|
| **Nvidia Clara** | Medical imaging pipeline | Process DICOM files from PACS |
| **MONAI** | Radiology AI (X-ray/CT/MRI) | Pre-trained models for image analysis |
| **CheXNet** | Chest X-ray detection | Pneumonia/pathology detection |
| **FastAI Medical** | Transfer learning | Retinal scans, pathology slides |
| **XGBoost** | Lab predictions | Predict trends from time-series labs |
| **scispaCy** | Medical NLP | Extract entities from clinical notes |
| **Neo4j** | Knowledge graph | Validate LLM outputs against medical ontologies |
| **OHDSI** | Clinical research data | Standardized health data for evidence retrieval |
| **i2b2** | Clinical data warehouse | Hospital analytics integration |
| **OpenMRS** | Open EHR system | EHR integration patterns |

### RPA & Automation (Roadmap)
- **pyautogui**: Desktop automation (mouse/keyboard control)
- **Playwright**: Browser automation (Epic web interface)
- **watchdog**: File system monitoring (USB device detection)
- **python-docx**: Word document generation
- **openpyxl**: Excel report generation
- **pydicom**: DICOM medical image handling

---

## Key Design Patterns

### 1. Cryptographic Integrity
All modifications to chain or audit log must update hashes:
```python
# Canonical JSON serialization
json.dumps(data, sort_keys=True, separators=(',', ':'))

# SHA-256 hash
hashlib.sha256(canonical.encode()).hexdigest()
```

### 2. Hospital WiFi Lock
**CRITICAL**: Never disable in production
```python
REQUIRE_WIFI_CHECK = True  # app.py line 16
HOSPITAL_SSID_KEYWORDS = ["hospital", "clinical", "healthcare"]
```

### 3. Zero-Cloud Architecture
- All inference happens locally
- No external API calls (except captive portal check)
- All PHI stays on hospital hardware

### 4. Physician-in-the-Loop
- E-signature required before logging
- Physician can modify recommendations
- All decisions labeled as "decision support"

---

## Development Workflows

### Adding a New Model to Chain

1. Create method in `MultiLLMChain`:
```python
def _run_new_model(self, patient_context, query, prev_step):
    prompt = f"""You are a [persona]...

    INPUTS: {prev_step.response}
    TASK: [specific task]
    """

    response = grok_query(prompt, max_tokens=250)
    step = ChainStep(
        "New Model",
        prompt,
        response,
        datetime.utcnow().isoformat() + "Z",
        self._get_last_hash(),
        "",
        confidence=0.85
    )
    step.step_hash = self._compute_step_hash(...)
    self.chain_history.append(step)
    return step
```

2. Update `run_chain()` to call new model (line 46)
3. Adjust confidence calculation in arbiter (line 129)
4. Update `export_chain()` if needed

### Modifying UI

When editing `app.py`:
1. Preserve WiFi check logic (lines 19-43)
2. Maintain dual-mode structure
3. Update audit logging to include new fields
4. Test both Fast and Chain modes

### Testing

**Run test suite**:
```bash
python -m unittest test_v2.py -v
```

**Test chain integrity**:
```python
from llm_chain import MultiLLMChain

chain = MultiLLMChain()
result = chain.run_chain(patient_context, query, evidence, bayesian)
assert chain.verify_chain() == True
```

**Test audit integrity**:
```python
from audit_log import verify_audit_integrity

result = verify_audit_integrity()
assert result['valid'] == True
```

---

## Configuration & Deployment

### Environment Variables

```bash
# Required
export GROK_MODEL_PATH="/models/llama-3.1-70b-instruct-awq"

# Optional (for development)
export REQUIRE_WIFI_CHECK=false  # NEVER in production!
```

### Hospital WiFi Configuration

Edit `app.py` line 14:
```python
HOSPITAL_SSID_KEYWORDS = ["YourHospital-Secure", "YourHospital-Clinical"]
```

### Launch Scripts

**Setup (one-time)**:
```bash
./setup.sh                # Full setup with model download
./setup.sh --skip-model   # Skip model download
```

**Launch**:
```bash
./launch_v2.sh                    # Default (port 8501)
./launch_v2.sh --port 8080        # Custom port
./launch_v2.sh --no-wifi-check    # Dev mode (NEVER in production)
```

**Manual launch**:
```bash
streamlit run app.py --server.port 8501
```

---

## Medical AI Best Practices

### When to Use Which Mode

**Use Fast Mode for**:
- Straightforward questions
- Time-sensitive decisions (< 3s required)
- High Bayesian confidence (> 90%)
- Low-risk scenarios

**Use Chain Mode for**:
- Complex pharmacokinetics
- High-risk medications (vancomycin, warfarin)
- Multiple comorbidities
- Uncertain Bayesian confidence (< 70%)
- Regulatory documentation needed

### Safety Considerations

1. **Never skip adversarial model** - Critical safety check
2. **Always verify chain integrity** - Ensures no tampering
3. **Require physician sign-off** - Human-in-the-loop
4. **Log complete chain** - Regulatory compliance
5. **Maintain zero-cloud** - PHI protection

---

## Security

### HIPAA Compliance

- **Network Isolation**: Hospital WiFi enforcement + firewall rules
- **Audit Trail**: Immutable blockchain-style logging
- **Access Control**: E-signature required
- **PHI Protection**: All data stays on-premises
- **Encryption**: SQLite database should be encrypted at rest

See `SECURITY.md` for complete security policy.

### Threat Model

**In Scope**:
- PHI exposure to external networks
- Audit log tampering
- Unauthorized access
- Prompt injection attacks

**Mitigations**:
- WiFi check + firewall rules
- Hash chain verification
- E-signature requirement
- Structured prompts (no raw user input)

---

## Common Tasks

### Running the Full System

```bash
# 1. Setup (one-time)
./setup.sh

# 2. Launch
./launch_v2.sh

# 3. Access UI
# http://localhost:8501

# 4. Choose mode and enter patient info

# 5. Review and sign recommendation
```

### Exporting Audit Trail

```python
from audit_log import export_audit_trail

export_audit_trail("audit_export_2025-11-18.json")
```

### Verifying Multi-LLM Chain

```python
from llm_chain import run_multi_llm_decision

result = run_multi_llm_decision(
    patient_context={'age': 72, 'gender': 'M', 'labs': 'Cr 1.8'},
    query="Safe vancomycin dose?",
    retrieved_cases=cases,
    bayesian_result={'prob_safe': 0.85, 'n_cases': 150}
)

print(f"Final recommendation: {result['final_recommendation']}")
print(f"Confidence: {result['final_confidence']:.1%}")
print(f"Chain verified: {result['chain_export']['chain_verified']}")
```

---

## Roadmap & Future Enhancements

### Phase 3: Multi-Agent Orchestration (v3.0)

**Goal**: Replace manual chain coordination with CrewAI role-based agents

**Implementation**:
```python
from crewai import Agent, Task, Crew

# Define specialized agents
kinetics_agent = Agent(
    role='Clinical Pharmacologist',
    goal='Calculate precise pharmacokinetics and dosing',
    backstory='Expert in PK/PD modeling with 20 years experience',
    llm=grok_query  # Use existing vLLM backend
)

adversarial_agent = Agent(
    role='Risk Analyst',
    goal='Identify all potential risks and contraindications',
    backstory='Paranoid devil\'s advocate focused on patient safety'
)

literature_agent = Agent(
    role='Clinical Researcher',
    goal='Validate recommendations against current evidence',
    backstory='Evidence-based medicine expert'
)

arbiter_agent = Agent(
    role='Attending Physician',
    goal='Synthesize inputs into final clinical decision',
    backstory='Senior physician with decision-making authority'
)

# Define workflow
crew = Crew(
    agents=[kinetics_agent, adversarial_agent, literature_agent, arbiter_agent],
    tasks=[kinetics_task, adversarial_task, literature_task, arbiter_task],
    process='sequential'  # Maintains current chain structure
)

result = crew.kickoff(inputs={'patient': patient_context, 'query': query})
```

**Benefits**:
- Agents can autonomously request more information from each other
- Built-in memory and context sharing
- Easier to add new specialized agents (e.g., Radiology agent, Lab agent)

**Timeline**: Q1 2026

---

### Phase 4: Medical Imaging AI (v4.0)

**Goal**: Integrate radiology AI for automated image analysis

**Components**:

1. **MONAI Integration** (`monai_analyzer.py`)
   ```python
   from monai.networks.nets import DenseNet121
   from monai.transforms import LoadImage, EnsureChannelFirst, ScaleIntensity

   class MedicalImageAnalyzer:
       def analyze_xray(self, dicom_path: str) -> Dict:
           """Analyze chest X-ray for pneumonia, effusion, etc."""
           image = LoadImage()(dicom_path)
           prediction = self.model(image)
           return {
               'findings': ['Right lower lobe opacity', 'Cardiomegaly'],
               'confidence': 0.87,
               'heatmap': heatmap_overlay
           }
   ```

2. **CheXNet Integration** (`chexnet_detector.py`)
   - Stanford's 121-layer DenseNet trained on ChestX-ray14
   - Detects 14 pathologies: Pneumonia, Effusion, Cardiomegaly, etc.
   - Generates visual heatmaps for radiologist review

3. **Workflow**:
   ```
   USB Device Plugged In (X-ray CD)
       ‚Üí watchdog detects new DICOM files
       ‚Üí Copy to server /incoming/
       ‚Üí MONAI/CheXNet analysis
       ‚Üí Results ‚Üí Radiology LLM Agent
       ‚Üí Auto-insert findings into SOAP note
   ```

**Integration with LLM Chain**:
- New **Radiology Agent** joins the crew
- Receives image analysis results
- Contributes radiologic perspective to clinical decision

**Hardware Requirements**:
- GPU: A100 40GB (can run alongside 70B LLM with MIG slicing)
- Storage: 500GB for DICOM cache

**Timeline**: Q2 2026

---

### Phase 5: RPA Desktop Automation (v5.0)

**Goal**: Auto-populate EHR systems from AI-generated SOAP notes

**Components**:

1. **Epic Automation** (`epic_rpa.py`)
   ```python
   from playwright.sync_api import sync_playwright
   import pyautogui

   class EpicRPA:
       def populate_soap_note(self, soap_text: str, mrn: str):
           """Opens Epic and auto-fills SOAP note"""
           with sync_playwright() as p:
               browser = p.chromium.launch()
               page = browser.new_page()

               # Navigate to Epic web interface
               page.goto('https://epic.hospital.local')
               page.fill('#mrn_input', mrn)
               page.click('#search_patient')

               # Open new note
               page.click('text=New Note')

               # Parse SOAP sections
               sections = self.parse_soap(soap_text)
               page.fill('#subjective', sections['subjective'])
               page.fill('#objective', sections['objective'])
               page.fill('#assessment', sections['assessment'])
               page.fill('#plan', sections['plan'])

               # Physician reviews before saving
               input("Press Enter after reviewing note...")
               page.click('#save_note')
   ```

2. **Docking Station Watcher** (`usb_watcher.py`)
   ```python
   from watchdog.observers import Observer
   from watchdog.events import FileSystemEventHandler

   class USBHandler(FileSystemEventHandler):
       def on_created(self, event):
           if event.src_path.endswith('.dcm'):  # DICOM file
               print(f"New X-ray detected: {event.src_path}")
               # Copy to server for MONAI analysis
               self.copy_to_server(event.src_path)
           elif event.src_path.endswith('.csv'):  # Lab results
               # Parse and send to XGBoost predictor
               self.process_labs(event.src_path)

   observer = Observer()
   observer.schedule(USBHandler(), path='/media/usb/', recursive=True)
   observer.start()
   ```

3. **Document Generation**:
   - `python-docx`: Generate Word reports for consultations
   - `openpyxl`: Create Excel spreadsheets for lab trends
   - `reportlab`: Generate PDF discharge summaries
   - `matplotlib`: Create charts for patient presentations

**Security Considerations**:
- RPA runs on doctor's workstation (not server)
- Requires physician approval before any EHR modification
- All actions logged to audit trail
- Session timeout after 5 minutes of inactivity

**Timeline**: Q3 2026

---

### Phase 6: Cross-Verification Engine (v6.0)

**Goal**: Validate LLM outputs against medical ontologies and enable agent debate

**Components**:

1. **AutoGen Multi-Agent Debate**
   ```python
   from autogen import AssistantAgent, UserProxyAgent

   # Kinetics agent proposes dose
   kinetics_agent = AssistantAgent("Kinetics")

   # Adversarial agent challenges
   adversarial_agent = AssistantAgent("Adversarial")

   # Enable debate
   result = kinetics_agent.initiate_chat(
       adversarial_agent,
       message="I recommend vancomycin 1500mg q12h based on CrCl 45"
   )
   # Adversarial: "That's too high for CrCl 45. AUC will exceed 600.
   #               Recommend 1250mg q12h with trough monitoring."
   ```

2. **Neo4j Knowledge Graph** (`knowledge_graph.py`)
   ```python
   from neo4j import GraphDatabase

   class MedicalKnowledgeGraph:
       def validate_recommendation(self, drug: str, condition: str) -> bool:
           """Check if drug is indicated for condition per SNOMED"""
           query = """
           MATCH (d:Drug {name: $drug})-[:INDICATED_FOR]->(c:Condition {name: $condition})
           RETURN count(d) > 0 as is_valid
           """
           result = self.driver.execute_query(query, drug=drug, condition=condition)
           return result[0]['is_valid']
   ```

3. **SNOMED/LOINC/ICD Validation**:
   - Verify all diagnoses map to valid ICD-10 codes
   - Check lab orders use correct LOINC codes
   - Validate drug names against RxNorm

4. **OHDSI Integration**:
   - Query Observational Health Data Sciences database
   - Retrieve real-world evidence for drug safety
   - Compare recommendations to cohort outcomes

**Timeline**: Q4 2026

---

### Phase 7: XGBoost Lab Predictions (v7.0)

**Goal**: Predict future lab values and trends

**Use Cases**:
- Predict Cr 24h after vancomycin dose
- Forecast INR response to warfarin adjustment
- Anticipate potassium changes with diuretic dosing

**Implementation**:
```python
import xgboost as xgb

class LabPredictor:
    def predict_creatinine_24h(self, patient_features: Dict) -> float:
        """Predict Cr 24 hours after nephrotoxic drug"""
        X = self.vectorize_features(patient_features)
        prediction = self.xgb_model.predict(X)
        return prediction[0]

# Integration with Kinetics Agent
predicted_cr = lab_predictor.predict_creatinine_24h(patient_context)
if predicted_cr > 1.5 * baseline_cr:
    kinetics_agent.adjust_dose(reason="Predicted AKI in 24h")
```

**Timeline**: Q1 2027

---

## Known Limitations

1. **WiFi check can be bypassed** - `REQUIRE_WIFI_CHECK=False` (remove in production)
2. **PIN in plaintext** - Integrate LDAP/AD for production
3. **No rate limiting** - Add throttling for production
4. **Model integrity not checked** - Verify SHA-256 of downloaded model
5. **No automatic audit backup** - Hospital must configure backups
6. **Hardcoded confidence values** - Literature model: 0.90, could be dynamic
7. **Sequential chain execution** - Adversarial and Literature could run in parallel

---

## Testing Strategy

### Unit Tests

- Test each model independently
- Mock `grok_query()` to avoid LLM calls
- Verify hash computation
- Test chain verification with tampered data

### Integration Tests

- Full chain execution with mocked LLM
- Audit log integration
- Mode switching (Fast ‚Üî Chain)

### Clinical Validation

- Retrospective case reviews
- Pharmacist agreement studies
- Comparison to guidelines
- IRB-approved prospective trials

---

## File Modification Guidelines

### When editing `llm_chain.py`:

1. **Preserve hash computation** (lines 36-40)
2. **Maintain chain verification** (lines 147-159)
3. **Keep confidence calculation** (line 129)
4. **Test integrity after changes**:
   ```bash
   python -m unittest test_v2.TestMultiLLMChain.test_chain_verification
   ```

### When editing `app.py`:

1. **Never remove WiFi check** in production builds
2. **Maintain dual-mode structure**
3. **Update audit_log calls** if changing data model
4. **Test both Fast and Chain modes**

### When editing `audit_log.py`:

1. **Never break hash chain** - all entries must link via `prev_hash`
2. **Maintain database schema compatibility**
3. **Test integrity verification** after changes
4. **Update migration scripts** if schema changes

---

## Git Workflow

**Current Branch**: `claude/claude-md-mi54iie7un3nrr8a-01HRQs6LTyAzZxG4x4hFy4J8`

### Branch Naming
- Feature branches: `claude/claude-md-<session-id>`
- Development on feature branches only

### Commit Style
- Imperative mood: "Add multi-LLM chain" (not "Added")
- Reference issues: "Fix #123: Resolve hash collision"
- Keep commits atomic and focused

### Push Protocol
```bash
git push -u origin claude/claude-md-<session-id>
```

---

## Documentation Map

- **README.md** - User-facing documentation, deployment guide
- **CLAUDE.md** - This file, AI assistant development guide
- **ROADMAP.md** - Product roadmap for v3.0-v7.0 (multi-agent, imaging, RPA)
- **MULTI_LLM_CHAIN.md** - Deep dive on chain architecture
- **MOBILE_DEPLOYMENT.md** - Mobile co-pilot deployment guide
- **QUICK_START_V2.md** - 5-minute quick start guide
- **CHANGELOG.md** - Version history
- **CONTRIBUTING.md** - Contribution guidelines
- **SECURITY.md** - Security policy and best practices

---

## Questions to Ask Users

Before making significant changes:

1. **Mode Selection**: When should system auto-route to Chain Mode?
2. **Confidence Thresholds**: What confidence triggers physician alert?
3. **Error Handling**: What happens if chain fails mid-execution?
4. **Caching**: Should identical queries return cached results?
5. **Integration**: Which EHR system (Epic, Cerner, other)?
6. **Deployment**: DGX Spark, A100 cluster, or other hardware?

---

## Related Documentation

- [vLLM Documentation](https://docs.vllm.ai/)
- [Streamlit Documentation](https://docs.streamlit.io/)
- [HIPAA Guidelines](https://www.hhs.gov/hipaa/index.html)
- [FDA Software as Medical Device](https://www.fda.gov/medical-devices/software-medical-device)

---

## Change Log

- **2025-11-18 (v2.0)**: Multi-LLM chain, dual-mode UI, enhanced documentation
- **2025-11-18 (v1.0)**: Initial release with single-LLM mode

---

## Contact & Contribution

**Repository**: https://github.com/bufirstrepo/Grok_doc_enteprise
**Issues**: https://github.com/bufirstrepo/Grok_doc_enteprise/issues
**Creator**: [@ohio_dino](https://twitter.com/ohio_dino)

When contributing:
- Maintain zero-cloud architecture
- Preserve HIPAA compliance
- Test both Fast and Chain modes
- Update this CLAUDE.md if modifying workflows

**Last Updated**: 2025-11-19
**Repository Status**: Production-ready v2.0 with enterprise roadmap
**File Count**: 22 files (11 Python, 2 Shell, 9 Markdown)
**Lines of Code**: ~4,100 (excluding generated files)
**Roadmap**: v3.0-v7.0 planned through Q1 2027
