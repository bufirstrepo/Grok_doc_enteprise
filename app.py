import streamlit as st
import requests
import os
import json
from datetime import datetime
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from local_inference import grok_query
from audit_log import log_decision, verify_audit_integrity
from bayesian_engine import bayesian_safety_assessment
from llm_chain import run_multi_llm_decision  # NEW in v2.0

# â”€â”€ CONFIGURATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
HOSPITAL_SSID_KEYWORDS = ["hospital", "clinical", "healthcare", "medical"]
CAPTIVE_PORTAL_CHECK = "http://captive.apple.com"
REQUIRE_WIFI_CHECK = True  # Set to False for local testing

# â”€â”€ FORCE HOSPITAL WIFI ONLY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def is_on_hospital_wifi():
    """
    Validates device is on hospital WiFi by checking captive portal.
    In production, enhance with:
    - Certificate pinning
    - MAC address whitelist
    - VPN tunnel verification
    """
    if not REQUIRE_WIFI_CHECK:
        return True

    
    try:
        r = requests.get(CAPTIVE_PORTAL_CHECK, timeout=3, allow_redirects=True)
        url_check = any(kw in r.url.lower() for kw in HOSPITAL_SSID_KEYWORDS)
        content_check = any(kw in r.text.lower() for kw in HOSPITAL_SSID_KEYWORDS)
        return url_check or content_check
    except Exception as e:
        st.error(f"Network check failed: {e}")
        return False

# Check WiFi before anything else
if not is_on_hospital_wifi():
    st.error("ğŸš« Grok Doc only works on hospital WiFi")
    st.info("Connect to Hospital-Clinical network to prevent PHI from leaving premises.")
    st.stop()

# â”€â”€ PAGE CONFIGURATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(
    page_title="Grok Doc v2.0 - Clinical AI Co-Pilot",
    page_icon="ğŸ©º",
    layout="centered"
)

st.title("ğŸ©º Grok Doc v2.0 â€” Multi-LLM Clinical AI")
st.caption("100% local â€¢ Zero cloud â€¢ Hospital WiFi only â€¢ HIPAA-compliant logging")

# â”€â”€ LOAD RESOURCES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource
def load_vector_db():
    """
    Loads local FAISS index and case database.
    If files don't exist, creates sample data for testing.
    """
    index_path = "case_index.faiss"
    cases_path = "cases_17k.jsonl"

    embedder = SentenceTransformer('all-MiniLM-L6-v2')

    
    embedder = SentenceTransformer('all-MiniLM-L6-v2')
    
    # Create sample data if files don't exist
    if not os.path.exists(index_path) or not os.path.exists(cases_path):
        st.warning("Creating sample case database for demo purposes...")
        from data_builder import create_sample_database
        create_sample_database(embedder)

    index = faiss.read_index(index_path)

    
    index = faiss.read_index(index_path)
    
    cases = []
    with open(cases_path, 'r') as f:
        for line in f:
            cases.append(json.loads(line))

    
    return index, cases, embedder

try:
    index, cases, embedder = load_vector_db()
    st.success(f"âœ“ Loaded {len(cases)} clinical cases locally")
except Exception as e:
    st.error(f"Failed to load case database: {e}")
    st.stop()

# â”€â”€ SIDEBAR: PATIENT CONTEXT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with st.sidebar:
    st.header("Patient Context")

    
    mrn = st.text_input(
        "Medical Record Number (MRN)",
        help="Required for audit trail"
    )

    age = st.slider("Age", 0, 120, 72)
    gender = st.selectbox("Gender", ["Male", "Female", "Other"])

    
    age = st.slider("Age", 0, 120, 72)
    gender = st.selectbox("Gender", ["Male", "Female", "Other"])
    
    chief = st.text_area(
        "Chief complaint / Clinical question",
        value="72 yo male, septic shock on vancomycin, Cr 2.9 â†’ 1.8. Safe trough?",
        height=100
    )

    
    labs = st.text_area(
        "Key labs / imaging (optional)",
        placeholder="Cr: 1.8, WBC: 14.2, Vanc trough: 18.3",
        height=80
    )

    st.divider()

    # NEW in v2.0: Mode selection
    st.subheader("ğŸ”¬ Analysis Mode")
    analysis_mode = st.radio(
        "Select reasoning mode:",
        options=["âš¡ Fast Mode (v1.0)", "ğŸ”— Multi-LLM Chain (v2.0)"],
        help="""
        **Fast Mode**: Single LLM call with Bayesian analysis (~2s)
        **Chain Mode**: 4-stage adversarial reasoning for critical decisions (~8s)
        """
    )

    st.divider()

    
    st.divider()
    
    col1, col2 = st.columns(2)
    with col1:
        submit = st.button("ğŸ” Analyze", type="primary", use_container_width=True)
    with col2:
        if st.button("ğŸ”’ Verify Audit Log", use_container_width=True):
            integrity = verify_audit_integrity()
            if integrity["valid"]:
                st.success(f"âœ“ {integrity['entries']} entries verified")
            else:
                st.error(f"âš ï¸ Tampering detected at entry {integrity['tampered_index']}")

# â”€â”€ MAIN ANALYSIS LOGIC â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if submit:
    if not mrn:
        st.error("MRN is required for audit compliance")
        st.stop()

    use_chain_mode = "Chain" in analysis_mode

    if use_chain_mode:
        with st.spinner("ğŸ”— Running Multi-LLM Chain: Kinetics â†’ Adversarial â†’ Literature â†’ Arbiter..."):
            start_time = datetime.now()

            # STEP 1: Vector retrieval
            query_text = f"{chief} {labs}".strip()
            query_embedding = embedder.encode([query_text])

            k = min(100, len(cases))
            distances, indices = index.search(query_embedding, k)

            retrieved_cases = [cases[idx] for idx in indices[0]]

            # STEP 2: Bayesian safety assessment
            bayesian_result = bayesian_safety_assessment(
                retrieved_cases=retrieved_cases,
                query_type="nephrotoxicity"
            )

            # STEP 3: Multi-LLM Chain (NEW in v2.0)
            patient_context = {
                'age': age,
                'gender': gender,
                'labs': labs if labs else 'Not provided'
            }

            try:
                chain_result = run_multi_llm_decision(
                    patient_context=patient_context,
                    query=chief,
                    retrieved_cases=retrieved_cases,
                    bayesian_result=bayesian_result
                )

                llm_response = chain_result['final_recommendation']
                confidence = chain_result['final_confidence']
                chain_steps = chain_result['chain_steps']
                chain_verified = chain_result['chain_export']['chain_verified']

            except Exception as e:
                st.error(f"Multi-LLM chain failed: {e}")
                llm_response = "Error: Could not generate recommendation. Please review manually."
                confidence = 0.0
                chain_steps = []
                chain_verified = False

            latency = (datetime.now() - start_time).total_seconds()

            # â”€â”€ DISPLAY CHAIN RESULTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            st.success(f"âš¡ Multi-LLM analysis complete in {latency:.2f}s")

            # Chain integrity badge
            if chain_verified:
                st.info("âœ“ Chain integrity verified via cryptographic hashing")
            else:
                st.warning("âš ï¸ Chain integrity check failed")

            # Metrics
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Final Confidence", f"{confidence:.1%}")
            with col2:
                st.metric("Cases Analyzed", bayesian_result['n_cases'])
            with col3:
                st.metric("Chain Steps", len(chain_steps))

            # Final recommendation
            st.markdown("### ğŸ¤– Final Clinical Recommendation")
            st.info(llm_response)

            # Chain reasoning breakdown
            with st.expander("ğŸ”— View Multi-LLM Reasoning Chain"):
                for step in chain_steps:
                    st.markdown(f"**{step['step']}** (Confidence: {step.get('confidence', 'N/A')})")
                    st.write(step['response'])
                    st.caption(f"Hash: {step['hash'][:16]}...")
                    st.divider()

            # Evidence summary
            with st.expander("ğŸ“Š View Retrieved Evidence"):
                for i, case in enumerate(retrieved_cases[:10]):
                    st.markdown(f"**Case {i+1}:** {case.get('summary', 'No summary available')}")

    else:  # Fast Mode (v1.0)
        with st.spinner("ğŸ”¬ Retrieving evidence â†’ Bayesian analysis â†’ LLM reasoning..."):
            start_time = datetime.now()

            # STEP 1: Vector retrieval
            query_text = f"{chief} {labs}".strip()
            query_embedding = embedder.encode([query_text])

            k = min(100, len(cases))
            distances, indices = index.search(query_embedding, k)

            retrieved_cases = [cases[idx] for idx in indices[0]]

            # STEP 2: Bayesian safety assessment
            bayesian_result = bayesian_safety_assessment(
                retrieved_cases=retrieved_cases,
                query_type="nephrotoxicity"
            )

            # STEP 3: Build prompt for LLM
            evidence_text = "\n".join([
                f"Case {i+1}: {case.get('summary', 'N/A')}"
                for i, case in enumerate(retrieved_cases[:20])
            ])

            prompt = f"""You are an expert intensivist providing a clinical decision support recommendation.

EVIDENCE FROM SIMILAR CASES:
{evidence_text[:6000]}

BAYESIAN ANALYSIS:
- Probability of safety: {bayesian_result['prob_safe']:.1%}
- 95% Credible Interval: [{bayesian_result['ci_low']:.1%}, {bayesian_result['ci_high']:.1%}]
- Based on {bayesian_result['n_cases']} similar cases

PATIENT CONTEXT:
Age: {age}, Gender: {gender}
Question: {chief}
Labs: {labs if labs else 'Not provided'}

Provide a concise recommendation (3-4 sentences max). Include:
1. Direct answer to the clinical question
2. Key safety considerations
3. Numerical probability estimate where appropriate
"""

            # STEP 4: Local LLM inference
            try:
                llm_response = grok_query(prompt)
                confidence = bayesian_result['prob_safe']
            except Exception as e:
                st.error(f"LLM inference failed: {e}")
                llm_response = "Error: Could not generate recommendation. Please review manually."
                confidence = 0.0

            latency = (datetime.now() - start_time).total_seconds()

            # â”€â”€ DISPLAY FAST MODE RESULTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            st.success(f"âš¡ Analysis complete in {latency:.2f}s")

            # Bayesian results
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Safety Probability", f"{bayesian_result['prob_safe']:.1%}")
            with col2:
                st.metric("Cases Analyzed", bayesian_result['n_cases'])
            with col3:
                st.metric("Confidence Interval",
                         f"{bayesian_result['ci_low']:.0%}-{bayesian_result['ci_high']:.0%}")

            # LLM recommendation
            st.markdown("### ğŸ¤– Clinical Recommendation")
            st.info(llm_response)

            # Evidence summary
            with st.expander("ğŸ“Š View Retrieved Evidence"):
                for i, case in enumerate(retrieved_cases[:10]):
                    st.markdown(f"**Case {i+1}:** {case.get('summary', 'No summary available')}")

    # â”€â”€ DOCTOR SIGN-OFF (Common to both modes) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.divider()
    st.markdown("### ğŸ‘¨â€âš•ï¸ Physician Review")

    col1, col2, col3 = st.columns(3)

    with col1:
        if st.button("âœ… Accept & Sign", type="primary", use_container_width=True):
            st.session_state['show_signature'] = True

    with col2:
        if st.button("âœï¸ Modify Recommendation", use_container_width=True):
            st.session_state['show_edit'] = True

    with col3:
        if st.button("âŒ Reject", use_container_width=True):
            st.warning("Recommendation rejected - not logged")

    # Signature modal
    if st.session_state.get('show_signature', False):
        with st.form("signature_form"):
            st.markdown("**Electronic Signature Required**")
            doctor_name = st.text_input("Physician Name", placeholder="Dr. Jane Smith")
            pin = st.text_input("PIN", type="password", help="4-6 digit PIN")

            col1, col2 = st.columns(2)
            with col1:
                sign_button = st.form_submit_button("Sign & Log", type="primary", use_container_width=True)
            with col2:
                cancel_button = st.form_submit_button("Cancel", use_container_width=True)

            if sign_button:
                if len(pin) < 4:
                    st.error("PIN must be at least 4 digits")
                elif not doctor_name:
                    st.error("Physician name required")
                else:
                    # Log to immutable audit trail
                    log_entry = log_decision(
                        mrn=mrn,
                        patient_context=f"Age: {age}, Gender: {gender}",
                        query=chief,
                        labs=labs,
                        response=llm_response,
                        doctor=doctor_name,
                        bayesian_prob=bayesian_result['prob_safe'],
                        latency=latency,
                        analysis_mode="chain" if use_chain_mode else "fast"  # NEW in v2.0
                    )

                    st.success(f"âœ“ Logged to immutable audit trail (Hash: {log_entry['hash'][:16]}...)")
                    st.session_state['show_signature'] = False
                    st.rerun()

            if cancel_button:
                st.session_state['show_signature'] = False
                st.rerun()

    # Edit modal
    if st.session_state.get('show_edit', False):
        with st.form("edit_form"):
            st.markdown("**Modify Recommendation**")
            edited_response = st.text_area(
                "Edited Recommendation",
                value=llm_response,
                height=150
            )

            col1, col2 = st.columns(2)
            with col1:
                save_button = st.form_submit_button("Save & Sign", type="primary", use_container_width=True)
            with col2:
                cancel_button = st.form_submit_button("Cancel", use_container_width=True)

            if save_button:
                st.session_state['edited_response'] = edited_response
                st.session_state['show_edit'] = False
                st.session_state['show_signature'] = True
                st.rerun()

            if cancel_button:
                st.session_state['show_edit'] = False
                st.rerun()

# â”€â”€ FOOTER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.divider()
st.caption("Grok Doc v2.0 | Multi-LLM Chain | 100% on-premises | Zero cloud dependency | Contact: @ohio_dino")
